{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pNfoXODGq4TV"
   },
   "source": [
    "## Assignment 3: Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qMr5xQgIq4TX"
   },
   "source": [
    "## Learning Outcomes\n",
    "\n",
    "In this assignment, you will do the following:\n",
    "\n",
    "* Import a dataset into the Databricks Spark environment\n",
    "\n",
    "* Create tables for the data imported\n",
    "\n",
    "* Perform basic data analysis using transformations and Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GgvpSXx_q4TX"
   },
   "source": [
    "**Setup Instructions**\n",
    "\n",
    "1. Read the article:\n",
    "> https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html (links to an external site)\n",
    "\n",
    "\n",
    "2. Download Data: You an either download the JSON file from the Assignment 3 home page in Learn under 'EXPECTATIONS AND ASSIGNMENT FILES' or you can download the file from Github to your computer: \n",
    "> https://github.com/dmatrix/examples/blob/master/spark/databricks/notebooks/py/data/iot_devices.json (links to an external site) Click on the raw data source -> right click -> click save as. The file will download locally and now you can import it to Databricks. **NOTE**: \n",
    "\n",
    "3. Import files: How to import your downloaded files (from step 3) to your Databricks cluster: https://www.projectpro.io/recipes/create-dataframe-from-json-file-read-data-from-dbfs-and-write-into-dbfs (links to an external site).\n",
    "\n",
    "\n",
    "4. Import the notebook below.  There is some data exploration already done in this notebook for your reference. For details on how to import the notebook above see: https://docs.databricks.com/user-guide/notebooks/index.html (links to an external site).\n",
    "> https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/5915990090493625/411609171004360/6085673883631125/latest.html\n",
    "\n",
    "\n",
    "5. Run it. **NOTE**: Don't forget to create a cluster and attach the imported notebook to it (left upper corner: button `detached`) before trying to run it.\n",
    "\n",
    "**Questions** (12 marks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "1. Explain the main differences between RDDs, Dataframes and Datasets (4 marks)\n",
    "*Answer:*\n",
    "*The RDDs was the primary user-facing API in Spark since its inception  and are defined as the distributed collection of the data elements without any schema. The Dataframes is  an immutable distributed collection of data organized into named columns. The Dataset is an extension of the Dataframe with more added features like type-safety and strongly-typed.*"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "2. Answer the following questions:\n",
    "\n",
    "2.1 How many sensor pads are reported to be from Poland (2 marks)\n",
    "*Answer:*\n",
    "*12,744*\n",
    "\n",
    "2.2 How many different LCDs (distinct colors) are present in the dataset (2 marks)\n",
    "*Answer:*\n",
    "*3*\n",
    "\n",
    "2.3 Find 5 countries that have the largest number of MAC devices used (2 marks)\n",
    "*Answer:*\n",
    "\n",
    "| Country | Count |\n",
    "|-------------------|-------|\n",
    "| United States     | 70405 |\n",
    "| China             | 14455 |\n",
    "| Japan             | 12100 |\n",
    "| Republic of Korea | 11879 |\n",
    "| Germany           | 7942  |\n",
    "\n",
    "\n",
    "2.4 Propose and try an interesting statistical test or machine learning model you could use to gain insight from this dataset. Note, you don't have to use Machine Learning for this question. You can apply any analysis to the data even using SparkSQL, Python visualization libraries to analyze the data. Another example cloud be to apply correlation functions or other Spark functions to analyze the data. (2 marks)\n",
    "\n",
    "|summary|     battery_level|         c02_level|  cca2|  cca3|    cn|       device_id|         device_name|          humidity|            ip|          latitude|   lcd|          longitude|  scale|              temp|           timestamp|\n",
    "|-------|------------------|------------------|------|------|------|----------------|--------------------|------------------|--------------|------------------|------|-------------------|-------|------------------|--------------------|\n",
    "|  count|            198164|            198164|198164|198164|198164|          198164|              198164|            198164|        198164|            198164|198164|             198164| 198164|            198164|              198164|\n",
    "|   mean|4.4997678690377665|1199.7639429967098|  null|  null|  null|         99082.5|                null| 61.99212773258513|          null|36.521156062652814|  null|-0.6459595082860664|   null|22.012787388223895|1.458444058246215...|\n",
    "| stddev|2.8733916884106163| 231.0600256290079|  null|  null|  null|57205.1637092317|                null|21.672313062313982|          null|17.907740712889353|  null|  88.72758217920092|   null| 7.209848253887115|  1708.1019445038235|\n",
    "|    min|                 0|               800|    AD|   ABW|      |               1|device-mac-100005...|                25|108.57.128.215|            -51.75| green|             -175.0|Celsius|                10|       1458444054093|\n",
    "|    25%|                 2|              1000|  null|  null|  null|           49522|                null|                43|          null|             35.69|  null|             -87.69|   null|                16|       1458444056844|\n",
    "|    50%|                 5|              1199|  null|  null|  null|           99092|                null|                62|          null|              38.0|  null|               4.89|   null|                22|       1458444058328|\n",
    "|    75%|                 7|              1400|  null|  null|  null|          148629|                null|                81|          null|              47.0|  null|             100.52|   null|                28|       1458444059835|\n",
    "|    max|                 9|              1599|    ZW|   ZWE| Ã…land|          198164|therm-stick-99995...|                99|   99.64.14.90|              72.0|yellow|             178.42|Celsius|                34|       1458444061098|\n",
    "\n",
    "\n",
    "![](temp_barchart.png)\n",
    "\n",
    "**NOTE**: You may use MLLib in 2.4: https://spark.apache.org/docs/latest/ml-guide.html. Marks are awarded for the idea and implementation of the test/ML model.\n",
    "\n",
    "Please submit the **published** notebook link in a word/pdf document. Do not submit HTML, Jupyter notebook, or archive (DBC) formats."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 3_v2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}